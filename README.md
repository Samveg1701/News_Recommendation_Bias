Datasets are available at the following link:
https://drive.google.com/drive/folders/1KkWXiC2tRPdRwioUUhI7r-QfqKZAdcpw?usp=sharing

# News Recommendation Biases in Large Language Models

This research project investigates potential biases exhibited by large language models (LLMs) when recommending news articles. The study aims to shed light on the factors influencing the recommendations made by LLMs and their potential implications on public discourse and information dissemination.

## What the project does

The project explores two primary research questions:

1. Real vs. Fake News Bias: Do LLMs exhibit a preference for recommending either real or fake news articles when provided with a dataset containing both types?

2. Sentiment Bias: What is the sentiment of news articles recommended by LLMs when presented with a dataset containing news articles with varying sentiments?

## Why the project is useful

As large language models gain prominence in various applications, including news recommendation systems, it is crucial to understand and mitigate potential biases that could lead to the dissemination of misinformation or the reinforcement of echo chambers. This research aims to shed light on these biases and inform the development of more robust and unbiased AI systems.



## Maintainers

This project is maintained by:

- [Aaryann Mavani]
- [Rochan Mohapatra]
- [Samveg Bhansali]

## Acknowledgments
